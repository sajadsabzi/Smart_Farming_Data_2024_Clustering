{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842b565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26470ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 0: Starting initial setup for Clustering Analysis\n",
      "All directories created or verified successfully.\n",
      "Dataset loaded successfully.\n",
      "Dataset shape: (2200, 23)\n",
      "\n",
      "Phase 1: Starting Exploratory Data Analysis (EDA)\n",
      "Plot saved: correlation_heatmap_all_features\n",
      "Performing PCA for initial data visualization...\n",
      "Plot saved: pca_visualization_initial\n",
      "Exploratory Data Analysis completed.\n",
      "\n",
      "Phase 1.5: Defining research scenarios\n",
      "Scenario 1: 22 features.\n",
      "Scenario 2: 22 features.\n",
      "\n",
      "Phase 2: Starting data preprocessing\n",
      "Preprocessing pipeline will be defined per scenario.\n",
      "\n",
      "Phase 3: Starting comprehensive model evaluation and comparison\n",
      "\n",
      "===== EVALUATING SCENARIO: With_Derived_Features =====\n",
      "--- Evaluating model: KMeans ---\n",
      "Optimal k for KMeans found: 2\n",
      "Plot saved: k_selection_KMeans_With_Derived_Features\n",
      "Plot saved: pca_clusters_KMeans_With_Derived_Features\n",
      "--- Evaluating model: Agglomerative ---\n",
      "Optimal k for Agglomerative found: 2\n",
      "Plot saved: k_selection_Agglomerative_With_Derived_Features\n",
      "Plot saved: pca_clusters_Agglomerative_With_Derived_Features\n",
      "--- Evaluating model: GMM ---\n",
      "Optimal k for GMM found: 2\n",
      "Plot saved: k_selection_GMM_With_Derived_Features\n",
      "Plot saved: pca_clusters_GMM_With_Derived_Features\n",
      "--- Evaluating model: DBSCAN ---\n",
      "Plot saved: pca_clusters_DBSCAN_With_Derived_Features\n",
      "\n",
      "===== EVALUATING SCENARIO: Original_Features_Only =====\n",
      "--- Evaluating model: KMeans ---\n",
      "Optimal k for KMeans found: 2\n",
      "Plot saved: k_selection_KMeans_Original_Features_Only\n",
      "Plot saved: pca_clusters_KMeans_Original_Features_Only\n",
      "--- Evaluating model: Agglomerative ---\n",
      "Optimal k for Agglomerative found: 2\n",
      "Plot saved: k_selection_Agglomerative_Original_Features_Only\n",
      "Plot saved: pca_clusters_Agglomerative_Original_Features_Only\n",
      "--- Evaluating model: GMM ---\n",
      "Optimal k for GMM found: 2\n",
      "Plot saved: k_selection_GMM_Original_Features_Only\n",
      "Plot saved: pca_clusters_GMM_Original_Features_Only\n",
      "--- Evaluating model: DBSCAN ---\n",
      "Plot saved: pca_clusters_DBSCAN_Original_Features_Only\n",
      "\n",
      "Comprehensive model comparison completed. Results saved.\n",
      "   Silhouette Score  Davies-Bouldin Score  Calinski-Harabasz Score  \\\n",
      "0          0.169722              1.927282                180.74377   \n",
      "1          0.169722              1.927282                180.74377   \n",
      "4          0.169722              1.927282                180.74377   \n",
      "5          0.169722              1.927282                180.74377   \n",
      "2          0.118138              3.019686                150.50650   \n",
      "6          0.118138              3.019686                150.50650   \n",
      "3               NaN                   NaN                      NaN   \n",
      "7               NaN                   NaN                      NaN   \n",
      "\n",
      "   Number of Clusters  Noise Points  Adjusted Rand Score          Model  \\\n",
      "0                   2             0             0.018508         KMeans   \n",
      "1                   2             0             0.018508  Agglomerative   \n",
      "4                   2             0             0.018508         KMeans   \n",
      "5                   2             0             0.018508  Agglomerative   \n",
      "2                   2             0             0.039131            GMM   \n",
      "6                   2             0             0.039131            GMM   \n",
      "3                   0          2200                  NaN         DBSCAN   \n",
      "7                   0          2200                  NaN         DBSCAN   \n",
      "\n",
      "                 Scenario  \n",
      "0   With_Derived_Features  \n",
      "1   With_Derived_Features  \n",
      "4  Original_Features_Only  \n",
      "5  Original_Features_Only  \n",
      "2   With_Derived_Features  \n",
      "6  Original_Features_Only  \n",
      "3   With_Derived_Features  \n",
      "7  Original_Features_Only  \n",
      "\n",
      "Phase 4: Starting selection of best model and cluster profiling\n",
      "Best performing model identified: KMeans under scenario 'With_Derived_Features'\n",
      "Metrics: Silhouette=0.170, Davies-Bouldin=1.927\n",
      "\n",
      "Cluster Sizes:\n",
      "Cluster\n",
      "0     200\n",
      "1    2000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of Crop Types per Cluster:\n",
      "label    apple  banana  blackgram  chickpea  coconut  coffee  cotton  grapes  \\\n",
      "Cluster                                                                        \n",
      "0          100       0          0         0        0       0       0     100   \n",
      "1            0     100        100       100      100     100     100       0   \n",
      "\n",
      "label    jute  kidneybeans  ...  mango  mothbeans  mungbean  muskmelon  \\\n",
      "Cluster                     ...                                          \n",
      "0           0            0  ...      0          0         0          0   \n",
      "1         100          100  ...    100        100       100        100   \n",
      "\n",
      "label    orange  papaya  pigeonpeas  pomegranate  rice  watermelon  \n",
      "Cluster                                                             \n",
      "0             0       0           0            0     0           0  \n",
      "1           100     100         100          100   100         100  \n",
      "\n",
      "[2 rows x 22 columns]\n",
      "Cluster profiling completed.\n",
      "\n",
      "Phase 5: Starting in-depth analysis and visualization of profiles\n",
      "Plot saved: cluster_profiles_radar_chart\n",
      "Creating boxplots for key features across clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_754086/3626803.py:390: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=profile_df, x='Cluster', y=feature, ax=axes[i], palette='viridis')\n",
      "/tmp/ipykernel_754086/3626803.py:390: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=profile_df, x='Cluster', y=feature, ax=axes[i], palette='viridis')\n",
      "/tmp/ipykernel_754086/3626803.py:390: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=profile_df, x='Cluster', y=feature, ax=axes[i], palette='viridis')\n",
      "/tmp/ipykernel_754086/3626803.py:390: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(data=profile_df, x='Cluster', y=feature, ax=axes[i], palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved: key_features_boxplot_by_cluster\n",
      "\n",
      "================================================================================\n",
      "Clustering script executed successfully! All results and artifacts are saved.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Comprehensive Data Analysis Script for Q1 Paper - Clustering Version\n",
    "# Objective: Discover Smart Farming Profiles via Multi-Algorithm Clustering\n",
    "# Author: AI Assistant (Adapted for Clustering)\n",
    "# Version: 1.1 - Corrected Imports\n",
    "# ==============================================================================\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 0: Initial Setup, Libraries, and Data Loading\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"Phase 0: Starting initial setup for Clustering Analysis\")\n",
    "\n",
    "# --- Import essential libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# --- Import sklearn tools ---\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "\n",
    "# --- Import clustering models ---\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# --- General Settings ---\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 7,\n",
    "    'axes.titlesize': 7,\n",
    "    'axes.labelsize': 7,\n",
    "    'xtick.labelsize': 7,\n",
    "    'ytick.labelsize': 7,\n",
    "    'legend.fontsize': 7,\n",
    "    'figure.titlesize': 7,\n",
    "    'figure.facecolor': 'white'\n",
    "})\n",
    "RANDOM_STATE = 42\n",
    "# The 'label' column will be used for external validation, not for clustering itself\n",
    "GROUND_TRUTH_COL = 'label'\n",
    "\n",
    "# --- Define Paths ---\n",
    "# !!! IMPORTANT: Please update BASE_DIR to your project's root directory !!!\n",
    "BASE_DIR = '/home/sajad/Sajad_test/Smart_Farming_Data_2024_Clustering/'\n",
    "FILE_NAME = 'Crop_recommendationV2.csv' # Assuming this is your SF24 dataset file\n",
    "DATA_PATH = os.path.join(BASE_DIR, 'dataset', FILE_NAME)\n",
    "\n",
    "RESULTS_PATH = os.path.join(BASE_DIR, 'clustering_results')\n",
    "EDA_PATH = os.path.join(BASE_DIR, 'EDA')\n",
    "MODELS_PATH = os.path.join(BASE_DIR, 'models_artifacts')\n",
    "COMPARISON_PATH = os.path.join(RESULTS_PATH, 'comparison')\n",
    "BEST_MODEL_ANALYSIS_PATH = os.path.join(RESULTS_PATH, 'best_model_analysis')\n",
    "\n",
    "paths_to_create = [RESULTS_PATH, EDA_PATH, MODELS_PATH, COMPARISON_PATH, BEST_MODEL_ANALYSIS_PATH]\n",
    "for path in paths_to_create:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "print(\"All directories created or verified successfully.\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def save_plot(fig, path, filename, width_cm=20):\n",
    "    \"\"\"Helper function to save plots in high-quality formats.\"\"\"\n",
    "    width_in = width_cm / 2.54\n",
    "    aspect_ratio = fig.get_figheight() / fig.get_figwidth()\n",
    "    fig.set_size_inches(width_in, width_in * aspect_ratio)\n",
    "    fig.savefig(os.path.join(path, f\"{filename}.svg\"), format='svg', bbox_inches='tight', dpi=300)\n",
    "    fig.savefig(os.path.join(path, f\"{filename}.pdf\"), format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Plot saved: {filename}\")\n",
    "\n",
    "def get_clustering_metrics(data, labels, ground_truth=None):\n",
    "    \"\"\"Calculate all relevant clustering evaluation metrics.\"\"\"\n",
    "    # Filter out noise points (label == -1) for metric calculation\n",
    "    valid_indices = labels != -1\n",
    "    if np.sum(valid_indices) < 2 or len(np.unique(labels[valid_indices])) < 2:\n",
    "        return {\n",
    "            'Silhouette Score': np.nan, 'Davies-Bouldin Score': np.nan,\n",
    "            'Calinski-Harabasz Score': np.nan, 'Adjusted Rand Score': np.nan,\n",
    "            'Number of Clusters': 0, 'Noise Points': len(labels) - np.sum(valid_indices)\n",
    "        }\n",
    "    \n",
    "    metrics = {\n",
    "        'Silhouette Score': silhouette_score(data[valid_indices], labels[valid_indices]),\n",
    "        'Davies-Bouldin Score': davies_bouldin_score(data[valid_indices], labels[valid_indices]),\n",
    "        'Calinski-Harabasz Score': calinski_harabasz_score(data[valid_indices], labels[valid_indices]),\n",
    "        'Number of Clusters': len(np.unique(labels[valid_indices])),\n",
    "        'Noise Points': len(labels) - np.sum(valid_indices)\n",
    "    }\n",
    "    if ground_truth is not None:\n",
    "        metrics['Adjusted Rand Score'] = adjusted_rand_score(ground_truth[valid_indices], labels[valid_indices])\n",
    "    else:\n",
    "        metrics['Adjusted Rand Score'] = np.nan\n",
    "    return metrics\n",
    "\n",
    "def plot_radar_chart(data, title, save_path, filename):\n",
    "    \"\"\"Plots a radar chart for cluster profiling.\"\"\"\n",
    "    labels = data.columns\n",
    "    num_vars = len(labels)\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        values = row.tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, label=f'Cluster {i}')\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels, size=8)\n",
    "    \n",
    "    plt.title(title, size=15, color='black', y=1.1)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    save_plot(fig, save_path, filename, width_cm=25)\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"!!!!!!!!!! CRITICAL ERROR !!!!!!!!!!!\\nDataset file not found at:\\n{DATA_PATH}\")\n",
    "    exit()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 1: Exploratory Data Analysis (EDA)\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 1: Starting Exploratory Data Analysis (EDA)\")\n",
    "df.describe().to_excel(os.path.join(EDA_PATH, \"descriptive_statistics.xlsx\"))\n",
    "\n",
    "numerical_features_all = df.select_dtypes(include=np.number).columns.tolist()\n",
    "corr_matrix = df[numerical_features_all].corr()\n",
    "fig, ax = plt.subplots(figsize=(24, 20))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".1f\", annot_kws={\"size\": 8})\n",
    "ax.set_title(\"Feature Correlation Matrix\", fontsize=16)\n",
    "save_plot(fig, EDA_PATH, 'correlation_heatmap_all_features', width_cm=40)\n",
    "\n",
    "# --- PCA for initial visualization ---\n",
    "print(\"Performing PCA for initial data visualization...\")\n",
    "temp_df = df.drop(columns=[GROUND_TRUTH_COL], errors='ignore')\n",
    "numerical_features = temp_df.select_dtypes(include=np.number).columns\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(temp_df[numerical_features])\n",
    "\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(df_pca[:, 0], df_pca[:, 1], alpha=0.5)\n",
    "ax.set_title('2D PCA of the Dataset')\n",
    "ax.set_xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "ax.set_ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "save_plot(fig, EDA_PATH, 'pca_visualization_initial', width_cm=20)\n",
    "print(\"Exploratory Data Analysis completed.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 1.5: Define Research Scenarios\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 1.5: Defining research scenarios\")\n",
    "all_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "derived_features = ['Temperature-Humidity Index', 'Nutrient Balance Ratio', 'Water Availability Index', 'Photosynthesis Potential', 'Soil Fertility Index']\n",
    "# Correcting derived feature names if they have different names in the CSV\n",
    "derived_features_in_df = [f for f in df.columns if any(sub in f for sub in ['THI', 'NBR', 'WAI', 'PP', 'SFI'])]\n",
    "if not derived_features_in_df: # Fallback if names are different\n",
    "    derived_features_in_df = derived_features\n",
    "    \n",
    "original_features = [f for f in all_features if f not in derived_features_in_df]\n",
    "\n",
    "scenarios = {\n",
    "    \"With_Derived_Features\": all_features,\n",
    "    \"Original_Features_Only\": original_features\n",
    "}\n",
    "print(f\"Scenario 1: {len(scenarios['With_Derived_Features'])} features.\")\n",
    "print(f\"Scenario 2: {len(scenarios['Original_Features_Only'])} features.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 2: Data Preprocessing\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 2: Starting data preprocessing\")\n",
    "if GROUND_TRUTH_COL not in df.columns:\n",
    "    print(f\"Warning: Ground truth column '{GROUND_TRUTH_COL}' not found. External validation will be skipped.\")\n",
    "    ground_truth_labels = None\n",
    "else:\n",
    "    ground_truth_labels = df[GROUND_TRUTH_COL]\n",
    "    df_for_clustering = df.drop(columns=[GROUND_TRUTH_COL])\n",
    "\n",
    "# We will create a preprocessor inside the loop for each scenario\n",
    "print(\"Preprocessing pipeline will be defined per scenario.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 3: Comprehensive Comparison of Clustering Models\n",
    "# ------------------------------------------------------------------------------\n",
    "print(\"\\nPhase 3: Starting comprehensive model evaluation and comparison\")\n",
    "models = {\n",
    "    'KMeans': KMeans(random_state=RANDOM_STATE, n_init=10),\n",
    "    'Agglomerative': AgglomerativeClustering(),\n",
    "    'GMM': GaussianMixture(random_state=RANDOM_STATE),\n",
    "    # DBSCAN parameters (eps, min_samples) need careful tuning. Using defaults as a start.\n",
    "    'DBSCAN': DBSCAN(eps=1.5, min_samples=5) \n",
    "}\n",
    "\n",
    "all_scenarios_metrics = []\n",
    "k_range = range(2, 11) # Range of clusters to test for KMeans and GMM\n",
    "\n",
    "for scenario_name, feature_list in scenarios.items():\n",
    "    print(f\"\\n===== EVALUATING SCENARIO: {scenario_name} =====\")\n",
    "    \n",
    "    X = df[feature_list]\n",
    "    \n",
    "    # Define preprocessor for the current set of features\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', StandardScaler(), numerical_features)],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"--- Evaluating model: {model_name} ---\")\n",
    "        \n",
    "        if model_name in ['KMeans', 'GMM', 'Agglomerative']:\n",
    "            # Find optimal k\n",
    "            k_metrics = []\n",
    "            for k in k_range:\n",
    "                if model_name == 'KMeans':\n",
    "                    model.set_params(n_clusters=k)\n",
    "                elif model_name == 'GMM':\n",
    "                    model.set_params(n_components=k)\n",
    "                elif model_name == 'Agglomerative':\n",
    "                    model.set_params(n_clusters=k)\n",
    "                \n",
    "                labels = model.fit_predict(X_processed)\n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    score = silhouette_score(X_processed, labels)\n",
    "                    k_metrics.append({'k': k, 'silhouette': score})\n",
    "            \n",
    "            if not k_metrics:\n",
    "                print(f\"Could not find a valid clustering for {model_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            k_metrics_df = pd.DataFrame(k_metrics)\n",
    "            best_k = k_metrics_df.loc[k_metrics_df['silhouette'].idxmax()]['k']\n",
    "            print(f\"Optimal k for {model_name} found: {int(best_k)}\")\n",
    "\n",
    "            # Plot k-selection graph\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(k_metrics_df['k'], k_metrics_df['silhouette'], marker='o')\n",
    "            ax.set_title(f'Silhouette Score for {model_name} ({scenario_name})')\n",
    "            ax.set_xlabel('Number of Clusters (k)')\n",
    "            ax.set_ylabel('Silhouette Score')\n",
    "            ax.axvline(x=best_k, color='r', linestyle='--', label=f'Optimal k = {int(best_k)}')\n",
    "            ax.legend()\n",
    "            save_plot(fig, COMPARISON_PATH, f'k_selection_{model_name}_{scenario_name}', width_cm=15)\n",
    "\n",
    "            # Rerun with best k\n",
    "            if model_name == 'KMeans': model.set_params(n_clusters=int(best_k))\n",
    "            elif model_name == 'GMM': model.set_params(n_components=int(best_k))\n",
    "            elif model_name == 'Agglomerative': model.set_params(n_clusters=int(best_k))\n",
    "            \n",
    "        labels = model.fit_predict(X_processed)\n",
    "        \n",
    "        # Save the model and labels\n",
    "        joblib.dump(model, os.path.join(MODELS_PATH, f\"{model_name}_{scenario_name}_model.joblib\"))\n",
    "        np.save(os.path.join(MODELS_PATH, f\"{model_name}_{scenario_name}_labels.npy\"), labels)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = get_clustering_metrics(X_processed, labels, ground_truth=ground_truth_labels)\n",
    "        metrics['Model'] = model_name\n",
    "        metrics['Scenario'] = scenario_name\n",
    "        all_scenarios_metrics.append(metrics)\n",
    "        \n",
    "        # Visualize clusters on PCA plot\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        unique_labels = np.unique(labels)\n",
    "        palette = sns.color_palette(\"hsv\", len(unique_labels))\n",
    "        for i, label in enumerate(unique_labels):\n",
    "            if label == -1:\n",
    "                # Plot noise points in black\n",
    "                cluster_color = 'k'\n",
    "                point_label = 'Noise'\n",
    "            else:\n",
    "                cluster_color = palette[i]\n",
    "                point_label = f'Cluster {label}'\n",
    "            \n",
    "            ax.scatter(df_pca[labels == label, 0], df_pca[labels == label, 1],\n",
    "                       c=[cluster_color], label=point_label, alpha=0.7, s=30)\n",
    "        \n",
    "        ax.set_title(f'Clusters by {model_name} on PCA ({scenario_name})')\n",
    "        ax.set_xlabel(f'Principal Component 1')\n",
    "        ax.set_ylabel(f'Principal Component 2')\n",
    "        ax.legend()\n",
    "        save_plot(fig, COMPARISON_PATH, f'pca_clusters_{model_name}_{scenario_name}', width_cm=20)\n",
    "\n",
    "results_df = pd.DataFrame(all_scenarios_metrics).sort_values(by='Silhouette Score', ascending=False)\n",
    "results_df.to_excel(os.path.join(COMPARISON_PATH, \"all_models_metrics_comparison.xlsx\"), index=False)\n",
    "print(\"\\nComprehensive model comparison completed. Results saved.\")\n",
    "print(results_df)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 4: Best Model Selection and Cluster Profiling\n",
    "# ------------------------------------------------------------------------------\n",
    "print(f\"\\nPhase 4: Starting selection of best model and cluster profiling\")\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No valid clustering results found. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "best_model_info = results_df.iloc[0]\n",
    "best_model_name = best_model_info['Model']\n",
    "best_scenario_name = best_model_info['Scenario']\n",
    "print(f\"Best performing model identified: {best_model_name} under scenario '{best_scenario_name}'\")\n",
    "print(f\"Metrics: Silhouette={best_model_info['Silhouette Score']:.3f}, Davies-Bouldin={best_model_info['Davies-Bouldin Score']:.3f}\")\n",
    "\n",
    "# Load the best model's labels\n",
    "best_labels = np.load(os.path.join(MODELS_PATH, f\"{best_model_name}_{best_scenario_name}_labels.npy\"))\n",
    "\n",
    "# Create a profiling dataframe\n",
    "profile_df = df.copy()\n",
    "profile_df['Cluster'] = best_labels\n",
    "\n",
    "# Exclude noise points from profiling if any\n",
    "profile_df = profile_df[profile_df['Cluster'] != -1]\n",
    "\n",
    "# Calculate cluster profiles (mean of each feature per cluster)\n",
    "cluster_profiles = profile_df.groupby('Cluster').mean(numeric_only=True)\n",
    "cluster_profiles.to_excel(os.path.join(BEST_MODEL_ANALYSIS_PATH, \"cluster_profiles_mean.xlsx\"))\n",
    "\n",
    "# Calculate cluster sizes and distribution of 'label'\n",
    "cluster_sizes = profile_df['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster Sizes:\")\n",
    "print(cluster_sizes)\n",
    "\n",
    "if ground_truth_labels is not None:\n",
    "    label_distribution = pd.crosstab(profile_df['Cluster'], profile_df[GROUND_TRUTH_COL])\n",
    "    label_distribution.to_excel(os.path.join(BEST_MODEL_ANALYSIS_PATH, \"cluster_label_distribution.xlsx\"))\n",
    "    print(\"\\nDistribution of Crop Types per Cluster:\")\n",
    "    print(label_distribution)\n",
    "\n",
    "print(\"Cluster profiling completed.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Phase 5: In-depth Analysis & Visualization of Best Model's Profiles\n",
    "# ------------------------------------------------------------------------------\n",
    "print(f\"\\nPhase 5: Starting in-depth analysis and visualization of profiles\")\n",
    "\n",
    "# --- Radar Chart for Cluster Profiles ---\n",
    "# Select key features for the radar chart for better readability\n",
    "key_profile_features = [\n",
    "    'N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'soil_moisture',\n",
    "    'sunlight_exposure', 'organic_matter', 'pest_pressure', 'fertilizer_usage',\n",
    "    'THI', 'NBR', 'WAI', 'PP', 'SFI'\n",
    "]\n",
    "# Use correct names from dataframe\n",
    "key_profile_features_in_df = [f for f in cluster_profiles.columns if f in key_profile_features or any(sub in f for sub in ['THI', 'NBR', 'WAI', 'PP', 'SFI'])]\n",
    "\n",
    "# Normalize data for radar chart (0-1 scaling)\n",
    "radar_data = cluster_profiles[key_profile_features_in_df]\n",
    "radar_data_normalized = (radar_data - radar_data.min()) / (radar_data.max() - radar_data.min())\n",
    "\n",
    "plot_radar_chart(\n",
    "    radar_data_normalized,\n",
    "    f'Normalized Profiles of Clusters (Model: {best_model_name})',\n",
    "    BEST_MODEL_ANALYSIS_PATH,\n",
    "    'cluster_profiles_radar_chart'\n",
    ")\n",
    "\n",
    "# --- Boxplots for Key Features per Cluster ---\n",
    "print(\"Creating boxplots for key features across clusters...\")\n",
    "features_for_boxplot = ['temperature', 'humidity', 'soil_moisture', 'SFI', 'WAI', 'pest_pressure']\n",
    "# Use correct names from dataframe\n",
    "features_for_boxplot_in_df = [f for f in profile_df.columns if f in features_for_boxplot or any(sub in f for sub in ['SFI', 'WAI'])]\n",
    "\n",
    "\n",
    "n_features = len(features_for_boxplot_in_df)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 6, n_rows * 5), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features_for_boxplot_in_df):\n",
    "    sns.boxplot(data=profile_df, x='Cluster', y=feature, ax=axes[i], palette='viridis')\n",
    "    axes[i].set_title(f'Distribution of {feature} by Cluster')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(n_features, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    "save_plot(fig, BEST_MODEL_ANALYSIS_PATH, 'key_features_boxplot_by_cluster', width_cm=30)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nClustering script executed successfully! All results and artifacts are saved.\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
